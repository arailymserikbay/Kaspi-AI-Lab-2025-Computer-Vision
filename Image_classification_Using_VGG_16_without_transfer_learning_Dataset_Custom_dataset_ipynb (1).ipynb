{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sENTNpeRuAGv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'validation': transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ],
      "metadata": {
        "id": "qu5X7zyivaTm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/drive/MyDrive/Dataset/ds2'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'validation']}"
      ],
      "metadata": {
        "id": "-Zkm9n0oxpUe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import dataset\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
        "                                             shuffle=True, num_workers=4)\n",
        "              for x in ['train', 'validation']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'validation']}\n",
        "class_names = image_datasets['train'].classes\n",
        "print(class_names)\n",
        "print(dataset_sizes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQ7vrQzaz3z_",
        "outputId": "58f9f634-4763-42b8-91c9-e43ff9f74949"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['clean', 'water']\n",
            "{'train': 700, 'validation': 150}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load vgg16 with random initialization\n",
        "model_scratch = timm.create_model(\n",
        "    'vgg16',\n",
        "    pretrained=False,  # Random weights\n",
        "    num_classes=2     # Classifier for your dataset\n",
        ")\n",
        "\n",
        "# All layers trainable (default)\n",
        "for param in model_scratch.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Move to device\n",
        "model_scratch = model_scratch.to(device)\n",
        "\n",
        "# Optional: print summary\n",
        "from torchsummary import summary\n",
        "summary(model_scratch, input_size=(3, 224, 224))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WJFKECCxpON",
        "outputId": "f60bd4a8-8d38-4aee-cc66-428ecd52b048"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
            "              ReLU-2         [-1, 64, 224, 224]               0\n",
            "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
            "              ReLU-4         [-1, 64, 224, 224]               0\n",
            "         MaxPool2d-5         [-1, 64, 112, 112]               0\n",
            "            Conv2d-6        [-1, 128, 112, 112]          73,856\n",
            "              ReLU-7        [-1, 128, 112, 112]               0\n",
            "            Conv2d-8        [-1, 128, 112, 112]         147,584\n",
            "              ReLU-9        [-1, 128, 112, 112]               0\n",
            "        MaxPool2d-10          [-1, 128, 56, 56]               0\n",
            "           Conv2d-11          [-1, 256, 56, 56]         295,168\n",
            "             ReLU-12          [-1, 256, 56, 56]               0\n",
            "           Conv2d-13          [-1, 256, 56, 56]         590,080\n",
            "             ReLU-14          [-1, 256, 56, 56]               0\n",
            "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
            "             ReLU-16          [-1, 256, 56, 56]               0\n",
            "        MaxPool2d-17          [-1, 256, 28, 28]               0\n",
            "           Conv2d-18          [-1, 512, 28, 28]       1,180,160\n",
            "             ReLU-19          [-1, 512, 28, 28]               0\n",
            "           Conv2d-20          [-1, 512, 28, 28]       2,359,808\n",
            "             ReLU-21          [-1, 512, 28, 28]               0\n",
            "           Conv2d-22          [-1, 512, 28, 28]       2,359,808\n",
            "             ReLU-23          [-1, 512, 28, 28]               0\n",
            "        MaxPool2d-24          [-1, 512, 14, 14]               0\n",
            "           Conv2d-25          [-1, 512, 14, 14]       2,359,808\n",
            "             ReLU-26          [-1, 512, 14, 14]               0\n",
            "           Conv2d-27          [-1, 512, 14, 14]       2,359,808\n",
            "             ReLU-28          [-1, 512, 14, 14]               0\n",
            "           Conv2d-29          [-1, 512, 14, 14]       2,359,808\n",
            "             ReLU-30          [-1, 512, 14, 14]               0\n",
            "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
            "           Conv2d-32           [-1, 4096, 1, 1]     102,764,544\n",
            "             ReLU-33           [-1, 4096, 1, 1]               0\n",
            "          Dropout-34           [-1, 4096, 1, 1]               0\n",
            "           Conv2d-35           [-1, 4096, 1, 1]      16,781,312\n",
            "             ReLU-36           [-1, 4096, 1, 1]               0\n",
            "          ConvMlp-37           [-1, 4096, 1, 1]               0\n",
            "AdaptiveAvgPool2d-38           [-1, 4096, 1, 1]               0\n",
            "          Flatten-39                 [-1, 4096]               0\n",
            "SelectAdaptivePool2d-40                 [-1, 4096]               0\n",
            "          Dropout-41                 [-1, 4096]               0\n",
            "           Linear-42                    [-1, 2]           8,194\n",
            "         Identity-43                    [-1, 2]               0\n",
            "   ClassifierHead-44                    [-1, 2]               0\n",
            "================================================================\n",
            "Total params: 134,268,738\n",
            "Trainable params: 134,268,738\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 218.71\n",
            "Params size (MB): 512.19\n",
            "Estimated Total Size (MB): 731.48\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_scratch.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "UH05xKKSxpL4"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for phase in ['train', 'validation']:\n",
        "        if phase == 'train':\n",
        "            model_scratch.train()\n",
        "        else:\n",
        "            model_scratch.eval()\n",
        "    running_loss = 0.0\n",
        "    running_corrects = 0\n",
        "\n",
        "    for inputs, labels in dataloaders[phase]:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with torch.set_grad_enabled(phase == 'train'):\n",
        "            outputs = model_scratch(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            if phase == 'train':\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        running_corrects += torch.sum(preds == labels.data)\n",
        "    epoch_loss = running_loss / dataset_sizes[phase]\n",
        "    epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "    print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "        phase, epoch_loss, epoch_acc))\n",
        "print('Training complete')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPNlt4wwxpJ5",
        "outputId": "3ec6cd8b-109a-4183-8ac2-db6face6582e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation Loss: 0.6958 Acc: 0.4600\n",
            "validation Loss: 0.6958 Acc: 0.4600\n",
            "validation Loss: 0.6958 Acc: 0.4600\n",
            "validation Loss: 0.6958 Acc: 0.4600\n",
            "validation Loss: 0.6958 Acc: 0.4600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_scratch.state_dict(), 'vgg16_scratch.pth')"
      ],
      "metadata": {
        "id": "hc_QhAVfxpH6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = models.vgg16()  # Initialize the model\n",
        "# model.load_state_dict(torch.load('./vgg16_cifar10_without_transferlearning.pth'))\n",
        "# model = model.to(device)\n",
        "\n",
        "\n",
        "from torchvision import models\n",
        "\n",
        "model = models.vgg16(pretrained=False)  # Initialize VGG16 without pretrained weights\n",
        "\n",
        "# Modify the classifier to match the saved model\n",
        "num_features = model.classifier[6].in_features\n",
        "features = list(model.classifier.children())[:-1]  # Remove last layer\n",
        "features.extend([nn.Linear(num_features, 10)])  # Adapt to your dataset, CIFAR10 has 10 classes\n",
        "model.classifier = nn.Sequential(*features)\n",
        "\n",
        "# Load the state dict (model weights)\n",
        "model.load_state_dict(torch.load('./vgg16_scratch.pth'))\n",
        "\n",
        "# Move the model to the appropriate device (GPU or CPU)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "7zTVY6JZ25s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# 1. Load the image\n",
        "image_path = '/content/drive/MyDrive/Dataset/ds2/test/clean/IMG_1085.JPG'  # Change this to the path of your image\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# 2. Preprocess the image\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize the image to 224x224 pixels\n",
        "    transforms.ToTensor(),  # Convert the image to a PyTorch tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize it\n",
        "])\n",
        "input_image = transform(image).unsqueeze(0)  # Add a batch dimension\n",
        "\n",
        "# 3. Put the model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# 4. Make a prediction\n",
        "with torch.no_grad():\n",
        "    # Move the input and model to GPU for speed if available\n",
        "    if torch.cuda.is_available():\n",
        "        input_image = input_image.to('cuda')\n",
        "        model.to('cuda')\n",
        "\n",
        "    output = model(input_image)\n",
        "\n",
        "# 5. Interpret the output\n",
        "_, predicted = torch.max(output, 1)  # Get the index of the highest log-probability\n",
        "\n",
        "# Get the class label (modify this part according to your dataset classes)\n",
        "class_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "  # Replace with your own class labels\n",
        "predicted_class = class_labels[predicted.item()]\n",
        "\n",
        "print(\"Predicted Class: \", predicted_class)"
      ],
      "metadata": {
        "id": "BX95zRAJ3YT6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}